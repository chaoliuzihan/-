{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.word2vec import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(name, nrows=None):\n",
    "    datasets={\n",
    "        'unlabeled_train':'unlabeledTrainData.tsv',\n",
    "        'labeled_train': 'labeledTrainData.tsv',\n",
    "        'test':'testData.tsv'\n",
    "    }\n",
    "    if name not in datasets:\n",
    "        raise ValueErroe(name)\n",
    "    data_file = os.path.join('data',datasets[name])\n",
    "    df = pd.read_csv(data_file,sep='\\t', escapechar='\\\\', nrows=nrows)\n",
    "    print('Number of review:{}'.format(len(df)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读入无标签数据\n",
    "用于训练生成word2vec词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of review:50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9999_0</td>\n",
       "      <td>Watching Time Chasers, it obvious that it was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45057_0</td>\n",
       "      <td>I saw this film about 20 years ago and remembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15561_0</td>\n",
       "      <td>Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7161_0</td>\n",
       "      <td>I went to see this film with a great deal of e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43971_0</td>\n",
       "      <td>Yes, I agree with everyone on this site this m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             review\n",
       "0   9999_0  Watching Time Chasers, it obvious that it was ...\n",
       "1  45057_0  I saw this film about 20 years ago and remembe...\n",
       "2  15561_0  Minor Spoilers<br /><br />In New York, Joan Ba...\n",
       "3   7161_0  I went to see this film with a great deal of e...\n",
       "4  43971_0  Yes, I agree with everyone on this site this m..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_dataset('unlabeled_train')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords=set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text,remove_stopwords=False):\n",
    "    text=BeautifulSoup(text,'html.parser').get_text()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ',text)\n",
    "    words = text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        words = [w for w in words if w not in eng_stopwords]\n",
    "    return words\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def print_call_counts(f):\n",
    "    n=0\n",
    "    def wrapped(*args, **kwargs):\n",
    "        nonlocal n\n",
    "        n +=1\n",
    "        if n%1000==1:\n",
    "            print('method {} called {} times'.format(f.__name__, n))\n",
    "        return f(*args, **kwargs)\n",
    "    return wrapped\n",
    "\n",
    "@print_call_counts\n",
    "def split_sentences(review):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = [clean_text(s) for s in raw_sentences if s]\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 350001 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/halxp/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/Users/halxp/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/halxp/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 351001 times\n",
      "method split_sentences called 352001 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/halxp/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 353001 times\n",
      "method split_sentences called 354001 times\n",
      "method split_sentences called 355001 times\n",
      "method split_sentences called 356001 times\n",
      "method split_sentences called 357001 times\n",
      "method split_sentences called 358001 times\n",
      "method split_sentences called 359001 times\n",
      "method split_sentences called 360001 times\n",
      "method split_sentences called 361001 times\n",
      "method split_sentences called 362001 times\n",
      "method split_sentences called 363001 times\n",
      "method split_sentences called 364001 times\n",
      "method split_sentences called 365001 times\n",
      "method split_sentences called 366001 times\n",
      "method split_sentences called 367001 times\n",
      "method split_sentences called 368001 times\n",
      "method split_sentences called 369001 times\n",
      "method split_sentences called 370001 times\n",
      "method split_sentences called 371001 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/halxp/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 372001 times\n",
      "method split_sentences called 373001 times\n",
      "method split_sentences called 374001 times\n",
      "method split_sentences called 375001 times\n",
      "method split_sentences called 376001 times\n",
      "method split_sentences called 377001 times\n",
      "method split_sentences called 378001 times\n",
      "method split_sentences called 379001 times\n",
      "method split_sentences called 380001 times\n",
      "method split_sentences called 381001 times\n",
      "method split_sentences called 382001 times\n",
      "method split_sentences called 383001 times\n",
      "method split_sentences called 384001 times\n",
      "method split_sentences called 385001 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/halxp/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 386001 times\n",
      "method split_sentences called 387001 times\n",
      "method split_sentences called 388001 times\n",
      "method split_sentences called 389001 times\n",
      "method split_sentences called 390001 times\n",
      "method split_sentences called 391001 times\n",
      "method split_sentences called 392001 times\n",
      "method split_sentences called 393001 times\n",
      "method split_sentences called 394001 times\n",
      "method split_sentences called 395001 times\n",
      "method split_sentences called 396001 times\n",
      "method split_sentences called 397001 times\n",
      "method split_sentences called 398001 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/halxp/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 399001 times\n",
      "CPU times: user 7min 14s, sys: 36.9 s, total: 7min 51s\n",
      "Wall time: 8min\n",
      "50000review-> 537851 sentences\n"
     ]
    }
   ],
   "source": [
    "%time sentences = sum(df.review.apply(split_sentences),[])\n",
    "print('{}review-> {} sentences'.format(len(df),len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300\n",
    "min_word_count=40\n",
    "num_workers=4\n",
    "context=10\n",
    "downsampling = 1e-3\n",
    "\n",
    "model_name='{}features_{}minwords_{}context.model'.format(num_features,min_word_count,context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-17 22:41:12,689 : INFO : collecting all words and their counts\n",
      "2018-06-17 22:41:12,692 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-06-17 22:41:12,752 : INFO : PROGRESS: at sentence #10000, processed 225072 words, keeping 17237 word types\n",
      "2018-06-17 22:41:12,820 : INFO : PROGRESS: at sentence #20000, processed 443536 words, keeping 24570 word types\n",
      "2018-06-17 22:41:12,883 : INFO : PROGRESS: at sentence #30000, processed 666343 words, keeping 29785 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-17 22:41:12,947 : INFO : PROGRESS: at sentence #40000, processed 886903 words, keeping 33939 word types\n",
      "2018-06-17 22:41:13,020 : INFO : PROGRESS: at sentence #50000, processed 1103863 words, keeping 37503 word types\n",
      "2018-06-17 22:41:13,093 : INFO : PROGRESS: at sentence #60000, processed 1327231 words, keeping 40738 word types\n",
      "2018-06-17 22:41:13,159 : INFO : PROGRESS: at sentence #70000, processed 1550828 words, keeping 43603 word types\n",
      "2018-06-17 22:41:13,228 : INFO : PROGRESS: at sentence #80000, processed 1772824 words, keeping 46155 word types\n",
      "2018-06-17 22:41:13,287 : INFO : PROGRESS: at sentence #90000, processed 1987492 words, keeping 48328 word types\n",
      "2018-06-17 22:41:13,350 : INFO : PROGRESS: at sentence #100000, processed 2210772 words, keeping 50551 word types\n",
      "2018-06-17 22:41:13,414 : INFO : PROGRESS: at sentence #110000, processed 2435496 words, keeping 52762 word types\n",
      "2018-06-17 22:41:13,479 : INFO : PROGRESS: at sentence #120000, processed 2658449 words, keeping 54893 word types\n",
      "2018-06-17 22:41:13,557 : INFO : PROGRESS: at sentence #130000, processed 2877962 words, keeping 56598 word types\n",
      "2018-06-17 22:41:13,624 : INFO : PROGRESS: at sentence #140000, processed 3098235 words, keeping 58352 word types\n",
      "2018-06-17 22:41:13,684 : INFO : PROGRESS: at sentence #150000, processed 3315370 words, keeping 60013 word types\n",
      "2018-06-17 22:41:13,758 : INFO : PROGRESS: at sentence #160000, processed 3536039 words, keeping 61691 word types\n",
      "2018-06-17 22:41:13,828 : INFO : PROGRESS: at sentence #170000, processed 3758385 words, keeping 63292 word types\n",
      "2018-06-17 22:41:13,914 : INFO : PROGRESS: at sentence #180000, processed 3979413 words, keeping 64846 word types\n",
      "2018-06-17 22:41:14,003 : INFO : PROGRESS: at sentence #190000, processed 4203546 words, keeping 66403 word types\n",
      "2018-06-17 22:41:14,066 : INFO : PROGRESS: at sentence #200000, processed 4429481 words, keeping 67924 word types\n",
      "2018-06-17 22:41:14,137 : INFO : PROGRESS: at sentence #210000, processed 4652920 words, keeping 69248 word types\n",
      "2018-06-17 22:41:14,200 : INFO : PROGRESS: at sentence #220000, processed 4870835 words, keeping 70567 word types\n",
      "2018-06-17 22:41:14,268 : INFO : PROGRESS: at sentence #230000, processed 5093104 words, keeping 71912 word types\n",
      "2018-06-17 22:41:14,339 : INFO : PROGRESS: at sentence #240000, processed 5311435 words, keeping 73234 word types\n",
      "2018-06-17 22:41:14,405 : INFO : PROGRESS: at sentence #250000, processed 5532195 words, keeping 74486 word types\n",
      "2018-06-17 22:41:14,471 : INFO : PROGRESS: at sentence #260000, processed 5751629 words, keeping 75693 word types\n",
      "2018-06-17 22:41:14,552 : INFO : PROGRESS: at sentence #270000, processed 5973493 words, keeping 76829 word types\n",
      "2018-06-17 22:41:14,613 : INFO : PROGRESS: at sentence #280000, processed 6191000 words, keeping 77953 word types\n",
      "2018-06-17 22:41:14,674 : INFO : PROGRESS: at sentence #290000, processed 6415980 words, keeping 79135 word types\n",
      "2018-06-17 22:41:14,740 : INFO : PROGRESS: at sentence #300000, processed 6634726 words, keeping 80229 word types\n",
      "2018-06-17 22:41:14,812 : INFO : PROGRESS: at sentence #310000, processed 6857605 words, keeping 81308 word types\n",
      "2018-06-17 22:41:14,880 : INFO : PROGRESS: at sentence #320000, processed 7077123 words, keeping 82421 word types\n",
      "2018-06-17 22:41:14,943 : INFO : PROGRESS: at sentence #330000, processed 7298667 words, keeping 83509 word types\n",
      "2018-06-17 22:41:15,013 : INFO : PROGRESS: at sentence #340000, processed 7516302 words, keeping 84445 word types\n",
      "2018-06-17 22:41:15,078 : INFO : PROGRESS: at sentence #350000, processed 7735101 words, keeping 85566 word types\n",
      "2018-06-17 22:41:15,148 : INFO : PROGRESS: at sentence #360000, processed 7956254 words, keeping 86544 word types\n",
      "2018-06-17 22:41:15,230 : INFO : PROGRESS: at sentence #370000, processed 8177574 words, keeping 87489 word types\n",
      "2018-06-17 22:41:15,303 : INFO : PROGRESS: at sentence #380000, processed 8395550 words, keeping 88515 word types\n",
      "2018-06-17 22:41:15,372 : INFO : PROGRESS: at sentence #390000, processed 8616518 words, keeping 89500 word types\n",
      "2018-06-17 22:41:15,440 : INFO : PROGRESS: at sentence #400000, processed 8835616 words, keeping 90470 word types\n",
      "2018-06-17 22:41:15,513 : INFO : PROGRESS: at sentence #410000, processed 9055384 words, keeping 91344 word types\n",
      "2018-06-17 22:41:15,576 : INFO : PROGRESS: at sentence #420000, processed 9276296 words, keeping 92245 word types\n",
      "2018-06-17 22:41:15,655 : INFO : PROGRESS: at sentence #430000, processed 9494459 words, keeping 93176 word types\n",
      "2018-06-17 22:41:15,721 : INFO : PROGRESS: at sentence #440000, processed 9719312 words, keeping 94119 word types\n",
      "2018-06-17 22:41:15,791 : INFO : PROGRESS: at sentence #450000, processed 9936915 words, keeping 94980 word types\n",
      "2018-06-17 22:41:15,853 : INFO : PROGRESS: at sentence #460000, processed 10160053 words, keeping 95781 word types\n",
      "2018-06-17 22:41:15,925 : INFO : PROGRESS: at sentence #470000, processed 10380740 words, keeping 96637 word types\n",
      "2018-06-17 22:41:15,988 : INFO : PROGRESS: at sentence #480000, processed 10599172 words, keeping 97471 word types\n",
      "2018-06-17 22:41:16,054 : INFO : PROGRESS: at sentence #490000, processed 10816559 words, keeping 98279 word types\n",
      "2018-06-17 22:41:16,123 : INFO : PROGRESS: at sentence #500000, processed 11032175 words, keeping 99064 word types\n",
      "2018-06-17 22:41:16,191 : INFO : PROGRESS: at sentence #510000, processed 11254508 words, keeping 99930 word types\n",
      "2018-06-17 22:41:16,310 : INFO : PROGRESS: at sentence #520000, processed 11481357 words, keeping 100836 word types\n",
      "2018-06-17 22:41:16,374 : INFO : PROGRESS: at sentence #530000, processed 11704018 words, keeping 101618 word types\n",
      "2018-06-17 22:41:16,422 : INFO : collected 102304 word types from a corpus of 11877522 raw words and 537851 sentences\n",
      "2018-06-17 22:41:16,424 : INFO : Loading a fresh vocabulary\n",
      "2018-06-17 22:41:16,552 : INFO : min_count=40 retains 13056 unique words (12% of original 102304, drops 89248)\n",
      "2018-06-17 22:41:16,553 : INFO : min_count=40 leaves 11401019 word corpus (95% of original 11877522, drops 476503)\n",
      "2018-06-17 22:41:16,641 : INFO : deleting the raw counts dictionary of 102304 items\n",
      "2018-06-17 22:41:16,646 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2018-06-17 22:41:16,648 : INFO : downsampling leaves estimated 8394665 word corpus (73.6% of prior 11401019)\n",
      "2018-06-17 22:41:16,651 : INFO : estimated required memory for 13056 words and 300 dimensions: 37862400 bytes\n",
      "2018-06-17 22:41:16,729 : INFO : resetting layer weights\n",
      "2018-06-17 22:41:17,086 : INFO : training model with 4 workers on 13056 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-06-17 22:41:18,102 : INFO : PROGRESS: at 1.04% examples, 436049 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:41:19,130 : INFO : PROGRESS: at 2.21% examples, 458613 words/s, in_qsize 6, out_qsize 1\n",
      "2018-06-17 22:41:20,147 : INFO : PROGRESS: at 3.40% examples, 467659 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-17 22:41:21,152 : INFO : PROGRESS: at 4.51% examples, 468195 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:41:22,156 : INFO : PROGRESS: at 5.68% examples, 471551 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-17 22:41:23,164 : INFO : PROGRESS: at 6.85% examples, 474525 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:41:24,184 : INFO : PROGRESS: at 7.99% examples, 474922 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:41:25,185 : INFO : PROGRESS: at 9.03% examples, 469270 words/s, in_qsize 6, out_qsize 1\n",
      "2018-06-17 22:41:26,191 : INFO : PROGRESS: at 10.02% examples, 463089 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:41:27,211 : INFO : PROGRESS: at 10.99% examples, 456901 words/s, in_qsize 8, out_qsize 1\n",
      "2018-06-17 22:41:28,217 : INFO : PROGRESS: at 12.10% examples, 457463 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-17 22:41:29,218 : INFO : PROGRESS: at 13.23% examples, 458668 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:41:30,226 : INFO : PROGRESS: at 14.27% examples, 456234 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:41:31,236 : INFO : PROGRESS: at 15.37% examples, 456580 words/s, in_qsize 6, out_qsize 1\n",
      "2018-06-17 22:41:32,269 : INFO : PROGRESS: at 16.59% examples, 459000 words/s, in_qsize 6, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-17 22:41:33,280 : INFO : PROGRESS: at 17.52% examples, 454743 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:41:34,299 : INFO : PROGRESS: at 18.74% examples, 456906 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:41:35,305 : INFO : PROGRESS: at 19.85% examples, 457614 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-17 22:41:36,327 : INFO : PROGRESS: at 21.02% examples, 458972 words/s, in_qsize 8, out_qsize 1\n",
      "2018-06-17 22:41:37,338 : INFO : PROGRESS: at 22.19% examples, 460442 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-17 22:41:38,358 : INFO : PROGRESS: at 23.36% examples, 461279 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:41:39,364 : INFO : PROGRESS: at 24.53% examples, 462593 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:41:40,368 : INFO : PROGRESS: at 25.74% examples, 464445 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-17 22:41:41,394 : INFO : PROGRESS: at 26.90% examples, 464861 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:41:42,400 : INFO : PROGRESS: at 28.15% examples, 467274 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:41:43,410 : INFO : PROGRESS: at 29.26% examples, 467029 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:41:44,417 : INFO : PROGRESS: at 30.44% examples, 467860 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:41:45,433 : INFO : PROGRESS: at 31.58% examples, 468014 words/s, in_qsize 6, out_qsize 1\n",
      "2018-06-17 22:41:46,435 : INFO : PROGRESS: at 32.69% examples, 467910 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:41:47,438 : INFO : PROGRESS: at 33.81% examples, 467769 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:41:48,466 : INFO : PROGRESS: at 34.98% examples, 467962 words/s, in_qsize 6, out_qsize 1\n",
      "2018-06-17 22:41:49,473 : INFO : PROGRESS: at 36.05% examples, 467358 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:41:50,483 : INFO : PROGRESS: at 37.11% examples, 466539 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:41:51,492 : INFO : PROGRESS: at 38.16% examples, 465556 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:41:52,517 : INFO : PROGRESS: at 39.32% examples, 465835 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-17 22:41:53,536 : INFO : PROGRESS: at 40.41% examples, 465570 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:41:54,541 : INFO : PROGRESS: at 41.46% examples, 464752 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:41:55,542 : INFO : PROGRESS: at 42.51% examples, 464214 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:41:56,550 : INFO : PROGRESS: at 43.58% examples, 463631 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:41:57,567 : INFO : PROGRESS: at 44.66% examples, 463314 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:41:58,594 : INFO : PROGRESS: at 45.69% examples, 462206 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:41:59,600 : INFO : PROGRESS: at 46.78% examples, 462051 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:00,611 : INFO : PROGRESS: at 47.84% examples, 461686 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:01,625 : INFO : PROGRESS: at 48.94% examples, 461469 words/s, in_qsize 6, out_qsize 1\n",
      "2018-06-17 22:42:02,653 : INFO : PROGRESS: at 50.03% examples, 461127 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:03,656 : INFO : PROGRESS: at 51.14% examples, 461208 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:04,665 : INFO : PROGRESS: at 52.33% examples, 461945 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-17 22:42:05,666 : INFO : PROGRESS: at 53.42% examples, 461729 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-17 22:42:06,671 : INFO : PROGRESS: at 54.40% examples, 460624 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:07,678 : INFO : PROGRESS: at 55.56% examples, 461091 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:08,693 : INFO : PROGRESS: at 56.77% examples, 461855 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:09,702 : INFO : PROGRESS: at 57.95% examples, 462375 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:10,709 : INFO : PROGRESS: at 59.15% examples, 463033 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:11,725 : INFO : PROGRESS: at 60.31% examples, 463448 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:12,743 : INFO : PROGRESS: at 61.37% examples, 462946 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:13,748 : INFO : PROGRESS: at 62.55% examples, 463442 words/s, in_qsize 6, out_qsize 1\n",
      "2018-06-17 22:42:14,751 : INFO : PROGRESS: at 63.67% examples, 463584 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-17 22:42:15,755 : INFO : PROGRESS: at 64.87% examples, 464310 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:16,757 : INFO : PROGRESS: at 66.07% examples, 464779 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:17,779 : INFO : PROGRESS: at 67.26% examples, 465318 words/s, in_qsize 6, out_qsize 1\n",
      "2018-06-17 22:42:18,799 : INFO : PROGRESS: at 68.48% examples, 465968 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:19,801 : INFO : PROGRESS: at 69.68% examples, 466513 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:20,812 : INFO : PROGRESS: at 70.89% examples, 467079 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-17 22:42:21,822 : INFO : PROGRESS: at 72.09% examples, 467538 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:22,840 : INFO : PROGRESS: at 73.30% examples, 468020 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:23,844 : INFO : PROGRESS: at 74.51% examples, 468591 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:24,847 : INFO : PROGRESS: at 75.70% examples, 468955 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:25,860 : INFO : PROGRESS: at 76.88% examples, 469326 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:26,868 : INFO : PROGRESS: at 78.02% examples, 469313 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-17 22:42:27,876 : INFO : PROGRESS: at 79.17% examples, 469407 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-17 22:42:28,880 : INFO : PROGRESS: at 80.30% examples, 469523 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:29,882 : INFO : PROGRESS: at 81.44% examples, 469643 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:30,891 : INFO : PROGRESS: at 82.65% examples, 470109 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:31,905 : INFO : PROGRESS: at 83.86% examples, 470520 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-17 22:42:32,908 : INFO : PROGRESS: at 85.03% examples, 470810 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:33,910 : INFO : PROGRESS: at 86.23% examples, 471184 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:34,910 : INFO : PROGRESS: at 87.41% examples, 471552 words/s, in_qsize 8, out_qsize 0\n",
      "2018-06-17 22:42:35,915 : INFO : PROGRESS: at 88.61% examples, 471984 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:36,930 : INFO : PROGRESS: at 89.73% examples, 471816 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:37,944 : INFO : PROGRESS: at 90.94% examples, 472183 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:38,959 : INFO : PROGRESS: at 92.15% examples, 472547 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:39,969 : INFO : PROGRESS: at 93.33% examples, 472744 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:40,994 : INFO : PROGRESS: at 94.53% examples, 472941 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:42,026 : INFO : PROGRESS: at 95.73% examples, 473096 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:43,043 : INFO : PROGRESS: at 96.94% examples, 473401 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:44,056 : INFO : PROGRESS: at 98.14% examples, 473646 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:45,070 : INFO : PROGRESS: at 99.35% examples, 473965 words/s, in_qsize 7, out_qsize 0\n",
      "2018-06-17 22:42:45,580 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-17 22:42:45,592 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-17 22:42:45,605 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-17 22:42:45,610 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-17 22:42:45,612 : INFO : training on 59387610 raw words (41971249 effective words) took 88.5s, 474168 effective words/s\n",
      "2018-06-17 22:42:45,613 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "print('Training model...')\n",
    "model =Word2Vec(sentences, workers=num_workers,\\\n",
    "                         size=num_features,min_count=min_word_count, \\\n",
    "                         window = context, sample=downsampling)\n",
    "\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-17 22:43:45,963 : INFO : saving Word2Vec object under models/300features_40minwords_10context.model, separately None\n",
      "2018-06-17 22:43:45,966 : INFO : not storing attribute syn0norm\n",
      "2018-06-17 22:43:45,968 : INFO : not storing attribute cum_table\n",
      "2018-06-17 22:43:46,543 : INFO : saved models/300features_40minwords_10context.model\n"
     ]
    }
   ],
   "source": [
    "model.save(os.path.join('models',model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kitchen\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match('man woman child kitchen'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6300984621047974),\n",
       " ('lady', 0.601123571395874),\n",
       " ('lad', 0.5779431462287903),\n",
       " ('guy', 0.5540357828140259),\n",
       " ('soldier', 0.5244459509849548),\n",
       " ('person', 0.5195951461791992),\n",
       " ('boy', 0.5006827712059021),\n",
       " ('chap', 0.49829626083374023),\n",
       " ('widow', 0.4941798150539398),\n",
       " ('men', 0.49312081933021545)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.779263973236084),\n",
       " ('horrible', 0.7369534969329834),\n",
       " ('atrocious', 0.7309613823890686),\n",
       " ('abysmal', 0.721771240234375),\n",
       " ('dreadful', 0.6979221105575562),\n",
       " ('horrid', 0.6907047033309937),\n",
       " ('horrendous', 0.677790641784668),\n",
       " ('appalling', 0.6498734354972839),\n",
       " ('lousy', 0.6388899087905884),\n",
       " ('amateurish', 0.632544219493866)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('awful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
